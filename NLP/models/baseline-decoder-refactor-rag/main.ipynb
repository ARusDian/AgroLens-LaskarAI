{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a99c492",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fdc6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "✅ Tokenizer saved to tokenizer-agrolens.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import pandas as pd\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# === Load data dari Excel ===\n",
    "excel_path = \"../../datasets/xlsx/Dataset Labeling Chatbot.xlsx\"\n",
    "sheet_names = [\"Blast\", \"Tungro\", \"Bacterial Blight\", \"BrownSpot\", \"LeafScald\"]\n",
    "\n",
    "texts = []\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "    prompts = df[\"prompt\"].dropna().astype(str).tolist()\n",
    "    responses = df[\"response\"].dropna().astype(str).tolist()\n",
    "    texts.extend(prompts + responses)\n",
    "\n",
    "# === Simpan ke file teks sebagai korpus tokenizer ===\n",
    "with open(\"tokenizer_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in texts:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# === Inisialisasi tokenizer BPE ===\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"],\n",
    ")\n",
    "\n",
    "# === Latih tokenizer ===\n",
    "tokenizer.train([\"tokenizer_corpus.txt\"], trainer)\n",
    "\n",
    "# === Tambahkan pemrosesan <s> dan </s> secara otomatis\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> <s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# === Tambahkan decoder agar hasil decode benar\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# === Simpan tokenizer ===\n",
    "tokenizer.save(\"tokenizer-agrolens.json\")\n",
    "print(\"✅ Tokenizer saved to tokenizer-agrolens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67402b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dataset.jsonl berhasil dibuat\n"
     ]
    }
   ],
   "source": [
    "excel_path = \"../../datasets/xlsx/Dataset Labeling Chatbot.xlsx\"\n",
    "sheet_names = [\"Blast\", \"Tungro\", \"Bacterial Blight\", \"BrownSpot\", \"LeafScald\"]\n",
    "\n",
    "all_data = []\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = str(row[\"prompt\"]).strip()\n",
    "        response = str(row[\"response\"]).strip()\n",
    "        if prompt and response:\n",
    "            all_data.append({\"prompt\": prompt, \"response\": response})\n",
    "\n",
    "# Simpan ke JSONL\n",
    "with open(\"dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ dataset.jsonl berhasil dibuat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d359a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'ĠApa', 'Ġitu', 'Ġpenyakit', 'Ġblast', '?', '</s>']\n",
      "[1, 247, 644, 140, 229, 26, 2]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer-agrolens.json\")\n",
    "enc = tokenizer.encode(\"Apa itu penyakit blast?\")\n",
    "print(enc.tokens)\n",
    "print(enc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0844",
   "metadata": {},
   "source": [
    "# GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645b29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_model * 4, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        attn = self.mha(x, x, attention_mask=mask, use_causal_mask=True)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        x = self.norm1(x + attn)\n",
    "\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout2(ffn_out, training=training)\n",
    "        return self.norm2(x + ffn_out)\n",
    "\n",
    "\n",
    "class AgroLensGPT(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_length=512,\n",
    "        d_model=256,\n",
    "        n_heads=4,\n",
    "        n_layers=4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.token_embed = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = layers.Embedding(max_length, d_model)\n",
    "        self.blocks = [DecoderBlock(d_model, n_heads, dropout) for _ in range(n_layers)]\n",
    "        self.final_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.output_head = layers.Dense(vocab_size)\n",
    "\n",
    "        # Precomputed causal mask (for max_length)\n",
    "        self.causal_mask = tf.linalg.band_part(tf.ones((max_length, max_length)), -1, 0)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B, T = tf.shape(x)[0], tf.shape(x)[1]\n",
    "        token_emb = self.token_embed(x)  # (B, T, d_model)\n",
    "        pos_indices = tf.range(start=0, limit=T)\n",
    "        pos_emb = self.pos_embed(pos_indices)[tf.newaxis, :, :]  # (1, T, d_model)\n",
    "\n",
    "        h = token_emb + pos_emb  # (B, T, d_model)\n",
    "        mask = self.causal_mask[:T, :T][tf.newaxis, tf.newaxis, :, :]  # (1, 1, T, T)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            h = block(h, training=training, mask=mask)\n",
    "\n",
    "        h = self.final_norm(h)\n",
    "        return self.output_head(h)  # (B, T, vocab_size)\n",
    "\n",
    "    def generate(self, tokenizer, prompt, max_new_tokens=50):\n",
    "        input_ids = tokenizer.encode(prompt).ids\n",
    "        input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(input_tensor, training=False)\n",
    "            next_token = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "            input_tensor = tf.concat(\n",
    "                [input_tensor, tf.expand_dims(next_token, axis=1)], axis=1\n",
    "            )\n",
    "\n",
    "            if next_token.numpy()[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "                break\n",
    "            if input_tensor.shape[1] >= self.max_length:\n",
    "                break\n",
    "\n",
    "        return tokenizer.decode(input_tensor[0].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fec44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749221379.095651    1753 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2863 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"agro_lens_gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"agro_lens_gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block (\u001b[38;5;33mDecoderBlock\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64, 8000)\n"
     ]
    }
   ],
   "source": [
    "model = AgroLensGPT(vocab_size=8000)\n",
    "display(model.summary())\n",
    "sample_input = tf.random.uniform((2, 64), minval=0, maxval=8000, dtype=tf.int32)\n",
    "logits = model(sample_input)\n",
    "print(logits.shape)  # Expected: (2, 64, 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ad101",
   "metadata": {},
   "source": [
    "# Loader dan Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d0427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# === Dataset loader ===\n",
    "class AgroDatasetTF(tf.data.Dataset):\n",
    "    def __new__(cls, path, tokenizer, max_len=512):\n",
    "        samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                text = f\"{data['prompt']} {data['response']}\"\n",
    "                ids = tokenizer.encode(text).ids[:max_len]\n",
    "                if len(ids) >= 2:\n",
    "                    input_ids = ids[:-1]\n",
    "                    labels = ids[1:]\n",
    "                    samples.append((input_ids, labels))\n",
    "\n",
    "        def gen():\n",
    "            for input_ids, labels in samples:\n",
    "                yield {\n",
    "                    \"input_ids\": tf.constant(input_ids, dtype=tf.int32),\n",
    "                    \"labels\": tf.constant(labels, dtype=tf.int32),\n",
    "                }\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_signature={\n",
    "                \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "                \"labels\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d8618d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noir/anaconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'agro_lens_gpt_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7f2b4c1a2b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7f2b4c1a2b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 22:52:02.839931: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 1: Loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 22:52:06.302471: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 2: Loss = nan\n",
      "📘 Epoch 3: Loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 22:52:13.050572: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 4: Loss = nan\n",
      "📘 Epoch 5: Loss = nan\n",
      "📘 Epoch 6: Loss = nan\n",
      "📘 Epoch 7: Loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 22:52:25.962528: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 8: Loss = nan\n",
      "📘 Epoch 9: Loss = nan\n",
      "📘 Epoch 10: Loss = nan\n",
      "📘 Epoch 11: Loss = nan\n",
      "📘 Epoch 12: Loss = nan\n",
      "📘 Epoch 13: Loss = nan\n",
      "📘 Epoch 14: Loss = nan\n",
      "📘 Epoch 15: Loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 22:52:54.583020: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 16: Loss = nan\n",
      "📘 Epoch 17: Loss = nan\n",
      "📘 Epoch 18: Loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📘 Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39msteps\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:419\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:385\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mArrayLike:\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Load tokenizer ===\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer-agrolens.json\")\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_LEN = 512\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "# === Load dataset ===\n",
    "dataset = AgroDatasetTF(\"dataset_ragstyle.jsonl\", tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "dataset = dataset.padded_batch(\n",
    "    batch_size=16,\n",
    "    padded_shapes={\"input_ids\": [None], \"labels\": [None]},\n",
    "    padding_values={\"input_ids\": PAD_TOKEN_ID, \"labels\": -100},  # ✅\n",
    ")\n",
    "# === Define model ===\n",
    "model = AgroLensGPT(vocab_size=VOCAB_SIZE, max_length=MAX_LEN)\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Training loop ===\n",
    "@tf.function\n",
    "def train_step(input_ids, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, training=True)\n",
    "        mask = tf.cast(labels != -100, tf.float32)\n",
    "        loss_vals = loss_fn(labels, logits)\n",
    "        loss = tf.reduce_sum(loss_vals * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for batch in dataset:\n",
    "        loss = train_step(batch[\"input_ids\"], batch[\"labels\"])\n",
    "        total_loss += loss.numpy()\n",
    "        steps += 1\n",
    "    print(f\"📘 Epoch {epoch}: Loss = {total_loss / steps:.4f}\")\n",
    "\n",
    "# === Save model weights ===\n",
    "model.save_weights(\"agrolens_model_tf_rag.weights.h5\")\n",
    "print(\"✅ Weights saved to agrolens_model_tf_rag.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Konstanta dan Load Model ---\n",
    "MODEL_PATH = \"agrolens_model_tf.weights.h5\"\n",
    "RETRIEVER_WEIGHTS = \"retriever_encoder_weights.weights.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer-agrolens.json\"\n",
    "VOCAB_SIZE = 8000\n",
    "MAX_LEN = 512\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ff9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌾 AgroLens Menjawab:\n",
      " Berikut adalah informasi terkait\n",
      "...\n",
      "Pertanyaan Apa itu penyakit blast?\n",
      "Jawaban Blast disebabkan oleh jamur...\n",
      " \n",
      " air dan menyebar melalui serangga penghisap, varietas tahan, dan membentuk appressorium, ini belum terbukti efektif dalam konteks internasional.\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Generate teks dari model AgroLensGPT berbasis prompt dan tokenizer.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Prompt input dari user.\n",
    "        max_new_tokens (int): Jumlah maksimum token baru yang dihasilkan.\n",
    "\n",
    "    Returns:\n",
    "        str: Hasil teks yang dihasilkan oleh model.\n",
    "    \"\"\"\n",
    "    # Tokenisasi prompt dan potong jika terlalu panjang\n",
    "    input_ids = tokenizer.encode(prompt).ids[:MAX_LEN]\n",
    "    input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Prediksi logit dari model\n",
    "        logits = model(input_tensor, training=False)\n",
    "\n",
    "        # Ambil token berikutnya (greedy decoding)\n",
    "        next_token = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "\n",
    "        # Gabungkan token ke input\n",
    "        input_tensor = tf.concat(\n",
    "            [input_tensor, tf.expand_dims(next_token, axis=1)], axis=1\n",
    "        )\n",
    "\n",
    "        # Stop jika token </s> ditemukan\n",
    "        if next_token.numpy()[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "\n",
    "        # Stop jika melebihi panjang maksimum\n",
    "        if input_tensor.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "\n",
    "    # Decode seluruh output menjadi teks\n",
    "    output_ids = input_tensor[0].numpy().tolist()\n",
    "    return tokenizer.decode(output_ids)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"\"\"<s> Berikut adalah informasi terkait:\n",
    "...\n",
    "Pertanyaan: Apa itu penyakit blast?\n",
    "Jawaban: Blast disebabkan oleh jamur...\n",
    "</s>\n",
    "\"\"\"\n",
    "    print(\"🌾 AgroLens Menjawab:\")\n",
    "    print(generate(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class AgroTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, vocab_size=8000, max_length=128, d_model=256, n_heads=4, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.pos_embed = layers.Embedding(input_dim=max_length, output_dim=d_model)\n",
    "\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(d_model * 4, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(d_model),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        pos = tf.expand_dims(pos, 0)\n",
    "        x = self.token_embed(x) + self.pos_embed(pos)\n",
    "\n",
    "        attn_output = self.attn(x, x, attention_mask=None, use_causal_mask=False)\n",
    "        x = self.ln1(x + self.dropout(attn_output, training=training))\n",
    "\n",
    "        ffn_output = self.ffn(x, training=training)\n",
    "        x = self.ln2(x + self.dropout(ffn_output, training=training))\n",
    "\n",
    "        x = tf.reduce_mean(x, axis=1)  # Global average pooling\n",
    "        return x  # shape: (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140283c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'agro_lens_gpt_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'agro_transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌾 AgroLens RAG Menjawab:\n",
      "[Context 1] Pengendalian hayati untuk penyakit blast bersifat lebih ramah lingkungan dan berbiaya rendah dibandi...\n",
      "[Context 2] Petani dapat menerapkan strategi seperti budidaya campuran varietas padi (multilines), penggunaan va...\n",
      "[Context 3] Ya, musim tanam yang jatuh di musim hujan atau transisi hujan-kemarau meningkatkan risiko blast, kar...\n",
      " Berikut adalah informasi terkait\n",
      "Pengendalian hayati untuk penyakit blast bersifat lebih ramah lingkungan dan berbiaya rendah dibandingkan penggunaan fungisida kimia. Agen hayati tidak mencemari lingkungan dan bisa menjadi alternatif yang berkelanjutan dalam jangka panjang.\n",
      "Petani dapat menerapkan strategi seperti budidaya campuran varietas padi (multilines), penggunaan varietas tahan spektrum luas, serta pengurangan ketergantungan pada fungisida melalui pendekatan rekayasa ekologi. Strategi ini meningkatkan hasil dan ketahanan penyakit secara berkelanjutan.\n",
      "Ya, musim tanam yang jatuh di musim hujan atau transisi hujan-kemarau meningkatkan risiko blast, karena spora jamur menyebar lebih cepat di kondisi lembab dan berembun.\n",
      "\n",
      "Pertanyaan Apa itu penyakit blast?\n",
      "Jawaban bu sorokiniana� lokus endemeringan pada merangs eradikasi kut jugaarah dunangat Peralihan selub\n"
     ]
    }
   ],
   "source": [
    "RETRIEVER_WEIGHTS = \"retriever_encoder_weights.weights.h5\"\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "# === Load GPT model ===\n",
    "model = AgroLensGPT(vocab_size=VOCAB_SIZE, max_length=MAX_LEN)\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "model.load_weights(MODEL_PATH)\n",
    "# === Load retriever encoder ===\n",
    "retriever_encoder = AgroTransformerEncoder(vocab_size=VOCAB_SIZE, max_length=MAX_LEN)\n",
    "retriever_encoder.build(input_shape=(None, MAX_LEN))\n",
    "retriever_encoder.load_weights(RETRIEVER_WEIGHTS)\n",
    "\n",
    "# === Load corpus passages (e.g. response list) ===\n",
    "with open(\"dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [json.loads(line)[\"response\"] for line in f if \"response\" in line]\n",
    "\n",
    "\n",
    "with open(\"rag_corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def detect_disease(prompt: str):\n",
    "    p = prompt.lower()\n",
    "    if \"blast\" in p:\n",
    "        return \"blast\"\n",
    "    elif \"tungro\" in p:\n",
    "        return \"tungro\"\n",
    "    # Tambahkan deteksi lain\n",
    "    return None\n",
    "\n",
    "\n",
    "def retrieve_top_k_contexts(query: str, top_k=3):\n",
    "    disease = detect_disease(query)\n",
    "    filtered = [c for c in corpus if c[\"disease\"] == disease]\n",
    "\n",
    "    query_ids = tokenizer.encode(query).ids[:MAX_LEN]\n",
    "    q_tensor = tf.constant([query_ids], dtype=tf.int32)\n",
    "    q_embed = retriever_encoder(q_tensor)\n",
    "\n",
    "    passage_embeddings = []\n",
    "    texts = []\n",
    "    for entry in filtered:\n",
    "        ids = tokenizer.encode(entry[\"text\"]).ids[:MAX_LEN]\n",
    "        t_tensor = tf.constant([ids], dtype=tf.int32)\n",
    "        emb = retriever_encoder(t_tensor)\n",
    "        passage_embeddings.append(emb[0].numpy())\n",
    "        texts.append(entry[\"text\"])\n",
    "\n",
    "    sims = tf.linalg.matmul(q_embed, tf.transpose(tf.constant(passage_embeddings)))\n",
    "    top_idx = tf.math.top_k(sims, k=min(top_k, len(texts))).indices.numpy()[0]\n",
    "    return [texts[i] for i in top_idx]\n",
    "\n",
    "\n",
    "# === Generate with GPT + context ===\n",
    "def generate_rag(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    top_contexts = retrieve_top_k_contexts(prompt)\n",
    "    for i, ctx in enumerate(top_contexts):\n",
    "        print(f\"[Context {i+1}] {ctx[:100]}...\")\n",
    "    full_prompt = (\n",
    "        \"Berikut adalah informasi terkait:\\n\"\n",
    "        + \"\\n\".join(top_contexts)\n",
    "        + f\"\\n\\nPertanyaan: {prompt}\\nJawaban:\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer.encode(full_prompt).ids[:MAX_LEN]\n",
    "    input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # logits.shape: [1, seq_len, vocab_size]\n",
    "        logits = model(input_tensor, training=False)\n",
    "        logits = logits[:, -1, :]  # ambil logit terakhir, shape: [1, vocab_size]\n",
    "\n",
    "        # Top-k sampling\n",
    "        k = 10\n",
    "        values, indices = tf.math.top_k(logits, k=k)  # [1, k]\n",
    "        next_token = tf.random.categorical(values, num_samples=1)  # [1, 1]\n",
    "        next_token = tf.gather(indices, next_token, batch_dims=1)  # [1, 1]\n",
    "        input_tensor = tf.concat(\n",
    "            [input_tensor, next_token], axis=1\n",
    "        )  # now both [1, seq_len]\n",
    "\n",
    "        if next_token.numpy()[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "        if input_tensor.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "\n",
    "    output_ids = input_tensor[0].numpy().tolist()\n",
    "    return tokenizer.decode(output_ids)\n",
    "\n",
    "\n",
    "# === Run test ===\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Apa itu penyakit blast?\"\n",
    "    print(\"🌾 AgroLens RAG Menjawab:\")\n",
    "    print(generate_rag(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454b4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53524a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170545fbea82488f8af2a4d0aac1fde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Apa itu penyakit blast?', description='❓ Pertanyaan:', layout=Layout(width='100%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    value=\"Apa itu penyakit blast?\",\n",
    "    placeholder=\"Tulis pertanyaan di sini...\",\n",
    "    description=\"❓ Pertanyaan:\",\n",
    "    layout=widgets.Layout(width=\"100%\"),\n",
    ")\n",
    "\n",
    "output_box = widgets.Output()\n",
    "generate_button = widgets.Button(\n",
    "    description=\"Jawab 🚀\", button_style=\"success\", layout=widgets.Layout(width=\"15%\")\n",
    ")\n",
    "\n",
    "\n",
    "def on_generate_clicked(b):\n",
    "    prompt = input_box.value\n",
    "    response = rag_generate(prompt)\n",
    "    output_box.clear_output()\n",
    "    with output_box:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"### 🧑 Kamu: \\n{prompt}\\n---\\n### 🌾 AgroLens Menjawab:\\n{response}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "generate_button.on_click(on_generate_clicked)\n",
    "\n",
    "# Tampilkan\n",
    "display(widgets.VBox([input_box, generate_button, output_box]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f5d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip <command> [options]\n",
      "\n",
      "Commands:\n",
      "  install                     Install packages.\n",
      "  download                    Download packages.\n",
      "  uninstall                   Uninstall packages.\n",
      "  freeze                      Output installed packages in requirements format.\n",
      "  inspect                     Inspect the python environment.\n",
      "  list                        List installed packages.\n",
      "  show                        Show information about installed packages.\n",
      "  check                       Verify installed packages have compatible dependencies.\n",
      "  config                      Manage local and global configuration.\n",
      "  search                      Search PyPI for packages.\n",
      "  cache                       Inspect and manage pip's wheel cache.\n",
      "  index                       Inspect information available from package indexes.\n",
      "  wheel                       Build wheels from your requirements.\n",
      "  hash                        Compute hashes of package archives.\n",
      "  completion                  A helper command used for command completion.\n",
      "  debug                       Show information useful for debugging.\n",
      "  help                        Show help for commands.\n",
      "\n",
      "General Options:\n",
      "  -h, --help                  Show help.\n",
      "  --debug                     Let unhandled exceptions propagate outside the\n",
      "                              main subroutine, instead of logging them to\n",
      "                              stderr.\n",
      "  --isolated                  Run pip in an isolated mode, ignoring\n",
      "                              environment variables and user configuration.\n",
      "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
      "                              exit with an error otherwise.\n",
      "  --python <python>           Run pip with the specified Python interpreter.\n",
      "  -v, --verbose               Give more output. Option is additive, and can be\n",
      "                              used up to 3 times.\n",
      "  -V, --version               Show version and exit.\n",
      "  -q, --quiet                 Give less output. Option is additive, and can be\n",
      "                              used up to 3 times (corresponding to WARNING,\n",
      "                              ERROR, and CRITICAL logging levels).\n",
      "  --log <path>                Path to a verbose appending log.\n",
      "  --no-input                  Disable prompting for input.\n",
      "  --keyring-provider <keyring_provider>\n",
      "                              Enable the credential lookup via the keyring\n",
      "                              library if user input is allowed. Specify which\n",
      "                              mechanism to use [disabled, import, subprocess].\n",
      "                              (default: disabled)\n",
      "  --proxy <proxy>             Specify a proxy in the form\n",
      "                              scheme://[user:passwd@]proxy.server:port.\n",
      "  --retries <retries>         Maximum number of retries each connection should\n",
      "                              attempt (default 5 times).\n",
      "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
      "  --exists-action <action>    Default action when a path already exists:\n",
      "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
      "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
      "                              even though it does not have valid or any HTTPS.\n",
      "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
      "                              provided, overrides the default. See 'SSL\n",
      "                              Certificate Verification' in pip documentation\n",
      "                              for more information.\n",
      "  --client-cert <path>        Path to SSL client certificate, a single file\n",
      "                              containing the private key and the certificate\n",
      "                              in PEM format.\n",
      "  --cache-dir <dir>           Store the cache data in <dir>.\n",
      "  --no-cache-dir              Disable the cache.\n",
      "  --disable-pip-version-check\n",
      "                              Don't periodically check PyPI to determine\n",
      "                              whether a new version of pip is available for\n",
      "                              download. Implied with --no-index.\n",
      "  --no-color                  Suppress colored output.\n",
      "  --no-python-version-warning\n",
      "                              Silence deprecation warnings for upcoming\n",
      "                              unsupported Pythons.\n",
      "  --use-feature <feature>     Enable new functionality, that may be backward\n",
      "                              incompatible.\n",
      "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
      "                              removed in the future.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
