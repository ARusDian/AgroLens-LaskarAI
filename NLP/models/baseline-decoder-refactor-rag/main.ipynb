{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a99c492",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fdc6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 00:14:32.403263: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-06 00:14:32.411819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749140072.421352   57076 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749140072.424410   57076 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749140072.432113   57076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749140072.432127   57076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749140072.432128   57076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749140072.432128   57076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-06 00:14:32.434998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "✅ Tokenizer saved to tokenizer-agrolens.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import pandas as pd\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# === Load data dari Excel ===\n",
    "excel_path = \"../../datasets/xlsx/Dataset Labeling Chatbot.xlsx\"\n",
    "sheet_names = [\"Blast\", \"Tungro\"]\n",
    "\n",
    "texts = []\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "    prompts = df[\"prompt\"].dropna().astype(str).tolist()\n",
    "    responses = df[\"response\"].dropna().astype(str).tolist()\n",
    "    texts.extend(prompts + responses)\n",
    "\n",
    "# === Simpan ke file teks sebagai korpus tokenizer ===\n",
    "with open(\"tokenizer_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in texts:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# === Inisialisasi tokenizer BPE ===\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"],\n",
    ")\n",
    "\n",
    "# === Latih tokenizer ===\n",
    "tokenizer.train([\"tokenizer_corpus.txt\"], trainer)\n",
    "\n",
    "# === Tambahkan pemrosesan <s> dan </s> secara otomatis\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> <s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# === Tambahkan decoder agar hasil decode benar\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# === Simpan tokenizer ===\n",
    "tokenizer.save(\"tokenizer-agrolens.json\")\n",
    "print(\"✅ Tokenizer saved to tokenizer-agrolens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67402b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dataset.jsonl berhasil dibuat\n"
     ]
    }
   ],
   "source": [
    "excel_path = \"../../datasets/xlsx/Dataset Labeling Chatbot.xlsx\"\n",
    "sheet_names = [\"Blast\", \"Tungro\"]\n",
    "\n",
    "all_data = []\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = str(row[\"prompt\"]).strip()\n",
    "        response = str(row[\"response\"]).strip()\n",
    "        if prompt and response:\n",
    "            all_data.append({\"prompt\": prompt, \"response\": response})\n",
    "\n",
    "# Simpan ke JSONL\n",
    "with open(\"dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ dataset.jsonl berhasil dibuat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d359a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'ĠApa', 'Ġitu', 'Ġpenyakit', 'Ġblast', '?', '</s>']\n",
      "[1, 176, 462, 158, 138, 22, 2]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer-agrolens.json\")\n",
    "enc = tokenizer.encode(\"Apa itu penyakit blast?\")\n",
    "print(enc.tokens)\n",
    "print(enc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0844",
   "metadata": {},
   "source": [
    "# GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645b29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_model * 4, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        attn = self.mha(x, x, attention_mask=mask, use_causal_mask=True)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        x = self.norm1(x + attn)\n",
    "\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout2(ffn_out, training=training)\n",
    "        return self.norm2(x + ffn_out)\n",
    "\n",
    "\n",
    "class AgroLensGPT(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_length=512,\n",
    "        d_model=256,\n",
    "        n_heads=4,\n",
    "        n_layers=4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.token_embed = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = layers.Embedding(max_length, d_model)\n",
    "        self.blocks = [DecoderBlock(d_model, n_heads, dropout) for _ in range(n_layers)]\n",
    "        self.final_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.output_head = layers.Dense(vocab_size)\n",
    "\n",
    "        # Precomputed causal mask (for max_length)\n",
    "        self.causal_mask = tf.linalg.band_part(tf.ones((max_length, max_length)), -1, 0)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B, T = tf.shape(x)[0], tf.shape(x)[1]\n",
    "        token_emb = self.token_embed(x)  # (B, T, d_model)\n",
    "        pos_indices = tf.range(start=0, limit=T)\n",
    "        pos_emb = self.pos_embed(pos_indices)[tf.newaxis, :, :]  # (1, T, d_model)\n",
    "\n",
    "        h = token_emb + pos_emb  # (B, T, d_model)\n",
    "        mask = self.causal_mask[:T, :T][tf.newaxis, tf.newaxis, :, :]  # (1, 1, T, T)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            h = block(h, training=training, mask=mask)\n",
    "\n",
    "        h = self.final_norm(h)\n",
    "        return self.output_head(h)  # (B, T, vocab_size)\n",
    "\n",
    "    def generate(self, tokenizer, prompt, max_new_tokens=50):\n",
    "        input_ids = tokenizer.encode(prompt).ids\n",
    "        input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(input_tensor, training=False)\n",
    "            next_token = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "            input_tensor = tf.concat(\n",
    "                [input_tensor, tf.expand_dims(next_token, axis=1)], axis=1\n",
    "            )\n",
    "\n",
    "            if next_token.numpy()[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "                break\n",
    "            if input_tensor.shape[1] >= self.max_length:\n",
    "                break\n",
    "\n",
    "        return tokenizer.decode(input_tensor[0].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fec44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749140075.905051   57076 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"agro_lens_gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"agro_lens_gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block (\u001b[38;5;33mDecoderBlock\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64, 8000)\n"
     ]
    }
   ],
   "source": [
    "model = AgroLensGPT(vocab_size=8000)\n",
    "display(model.summary())\n",
    "sample_input = tf.random.uniform((2, 64), minval=0, maxval=8000, dtype=tf.int32)\n",
    "logits = model(sample_input)\n",
    "print(logits.shape)  # Expected: (2, 64, 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ad101",
   "metadata": {},
   "source": [
    "# Loader dan Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# === Dataset loader ===\n",
    "class AgroDatasetTF(tf.data.Dataset):\n",
    "    def __new__(cls, path, tokenizer, max_len=512):\n",
    "        samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                text = f\"{data['prompt']} {data['response']}\"\n",
    "                ids = tokenizer.encode(text).ids[:max_len]\n",
    "                if len(ids) >= 2:\n",
    "                    input_ids = ids[:-1]\n",
    "                    labels = ids[1:]\n",
    "                    samples.append((input_ids, labels))\n",
    "\n",
    "        def gen():\n",
    "            for input_ids, labels in samples:\n",
    "                yield {\n",
    "                    \"input_ids\": tf.constant(input_ids, dtype=tf.int32),\n",
    "                    \"labels\": tf.constant(labels, dtype=tf.int32),\n",
    "                }\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_signature={\n",
    "                \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "                \"labels\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d8618d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'agro_lens_gpt_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 1: Loss = nan\n",
      "📘 Epoch 2: Loss = nan\n",
      "📘 Epoch 3: Loss = nan\n",
      "📘 Epoch 4: Loss = nan\n",
      "📘 Epoch 5: Loss = nan\n",
      "📘 Epoch 6: Loss = nan\n",
      "📘 Epoch 7: Loss = nan\n",
      "📘 Epoch 8: Loss = nan\n",
      "📘 Epoch 9: Loss = nan\n",
      "📘 Epoch 10: Loss = nan\n",
      "📘 Epoch 11: Loss = nan\n",
      "📘 Epoch 12: Loss = nan\n",
      "📘 Epoch 13: Loss = nan\n",
      "📘 Epoch 14: Loss = nan\n",
      "📘 Epoch 15: Loss = nan\n",
      "📘 Epoch 16: Loss = nan\n",
      "📘 Epoch 17: Loss = nan\n",
      "📘 Epoch 18: Loss = nan\n",
      "📘 Epoch 19: Loss = nan\n",
      "📘 Epoch 20: Loss = nan\n",
      "📘 Epoch 21: Loss = nan\n",
      "📘 Epoch 22: Loss = nan\n",
      "📘 Epoch 23: Loss = nan\n",
      "📘 Epoch 24: Loss = nan\n",
      "📘 Epoch 25: Loss = nan\n",
      "📘 Epoch 26: Loss = nan\n",
      "📘 Epoch 27: Loss = nan\n",
      "📘 Epoch 28: Loss = nan\n",
      "📘 Epoch 29: Loss = nan\n",
      "📘 Epoch 30: Loss = nan\n",
      "📘 Epoch 31: Loss = nan\n",
      "📘 Epoch 32: Loss = nan\n",
      "📘 Epoch 33: Loss = nan\n",
      "📘 Epoch 34: Loss = nan\n",
      "📘 Epoch 35: Loss = nan\n",
      "📘 Epoch 36: Loss = nan\n",
      "📘 Epoch 37: Loss = nan\n",
      "📘 Epoch 38: Loss = nan\n",
      "📘 Epoch 39: Loss = nan\n",
      "📘 Epoch 40: Loss = nan\n",
      "📘 Epoch 41: Loss = nan\n",
      "📘 Epoch 42: Loss = nan\n",
      "📘 Epoch 43: Loss = nan\n",
      "📘 Epoch 44: Loss = nan\n",
      "📘 Epoch 45: Loss = nan\n",
      "📘 Epoch 46: Loss = nan\n",
      "📘 Epoch 47: Loss = nan\n",
      "📘 Epoch 48: Loss = nan\n",
      "📘 Epoch 49: Loss = nan\n",
      "📘 Epoch 50: Loss = nan\n",
      "📘 Epoch 51: Loss = nan\n",
      "📘 Epoch 52: Loss = nan\n",
      "📘 Epoch 53: Loss = nan\n",
      "📘 Epoch 54: Loss = nan\n",
      "📘 Epoch 55: Loss = nan\n",
      "📘 Epoch 56: Loss = nan\n",
      "📘 Epoch 57: Loss = nan\n",
      "📘 Epoch 58: Loss = nan\n",
      "📘 Epoch 59: Loss = nan\n",
      "📘 Epoch 60: Loss = nan\n",
      "📘 Epoch 61: Loss = nan\n",
      "📘 Epoch 62: Loss = nan\n",
      "📘 Epoch 63: Loss = nan\n",
      "📘 Epoch 64: Loss = nan\n",
      "📘 Epoch 65: Loss = nan\n",
      "📘 Epoch 66: Loss = nan\n",
      "📘 Epoch 67: Loss = nan\n",
      "📘 Epoch 68: Loss = nan\n",
      "📘 Epoch 69: Loss = nan\n",
      "📘 Epoch 70: Loss = nan\n",
      "📘 Epoch 71: Loss = nan\n",
      "📘 Epoch 72: Loss = nan\n",
      "📘 Epoch 73: Loss = nan\n",
      "📘 Epoch 74: Loss = nan\n",
      "📘 Epoch 75: Loss = nan\n",
      "📘 Epoch 76: Loss = nan\n",
      "📘 Epoch 77: Loss = nan\n",
      "📘 Epoch 78: Loss = nan\n",
      "📘 Epoch 79: Loss = nan\n",
      "📘 Epoch 80: Loss = nan\n",
      "📘 Epoch 81: Loss = nan\n",
      "📘 Epoch 82: Loss = nan\n",
      "📘 Epoch 83: Loss = nan\n",
      "📘 Epoch 84: Loss = nan\n",
      "📘 Epoch 85: Loss = nan\n",
      "📘 Epoch 86: Loss = nan\n",
      "📘 Epoch 87: Loss = nan\n",
      "📘 Epoch 88: Loss = nan\n",
      "📘 Epoch 89: Loss = nan\n",
      "📘 Epoch 90: Loss = nan\n",
      "📘 Epoch 91: Loss = nan\n",
      "📘 Epoch 92: Loss = nan\n",
      "📘 Epoch 93: Loss = nan\n",
      "📘 Epoch 94: Loss = nan\n",
      "📘 Epoch 95: Loss = nan\n",
      "📘 Epoch 96: Loss = nan\n",
      "📘 Epoch 97: Loss = nan\n",
      "📘 Epoch 98: Loss = nan\n",
      "📘 Epoch 99: Loss = nan\n",
      "📘 Epoch 100: Loss = nan\n",
      "✅ Weights saved to agrolens_model_tf_rag.weights.h5\n"
     ]
    }
   ],
   "source": [
    "# === Load tokenizer ===\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer-agrolens.json\")\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_LEN = 512\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "# === Load dataset ===\n",
    "dataset = AgroDatasetTF(\"dataset_ragstyle.jsonl\", tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "dataset = dataset.padded_batch(\n",
    "    batch_size=16,\n",
    "    padded_shapes={\"input_ids\": [None], \"labels\": [None]},\n",
    "    padding_values={\"input_ids\": PAD_TOKEN_ID, \"labels\": -100},  # ✅\n",
    ")\n",
    "# === Define model ===\n",
    "model = AgroLensGPT(vocab_size=VOCAB_SIZE, max_length=MAX_LEN)\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Training loop ===\n",
    "@tf.function\n",
    "def train_step(input_ids, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, training=True)\n",
    "        mask = tf.cast(labels != -100, tf.float32)\n",
    "        loss_vals = loss_fn(labels, logits)\n",
    "        loss = tf.reduce_sum(loss_vals * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for batch in dataset:\n",
    "        loss = train_step(batch[\"input_ids\"], batch[\"labels\"])\n",
    "        total_loss += loss.numpy()\n",
    "        steps += 1\n",
    "    print(f\"📘 Epoch {epoch}: Loss = {total_loss / steps:.4f}\")\n",
    "\n",
    "# === Save model weights ===\n",
    "model.save_weights(\"agrolens_model_tf_rag.weights.h5\")\n",
    "print(\"✅ Weights saved to agrolens_model_tf_rag.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Konstanta dan Load Model ---\n",
    "MODEL_PATH = \"agrolens_model_tf.weights.h5\"\n",
    "RETRIEVER_WEIGHTS = \"retriever_encoder_weights.weights.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer-agrolens.json\"\n",
    "VOCAB_SIZE = 8000\n",
    "MAX_LEN = 512\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a45ff9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌾 AgroLens Menjawab:\n",
      " Berikut adalah informasi terkait\n",
      "...\n",
      "Pertanyaan Apa itu penyakit blast?\n",
      "Jawaban Blast disebabkan oleh jamur...\n",
      " \n",
      " air dan menyebar melalui serangga penghisap, varietas tahan, dan membentuk appressorium, ini belum terbukti efektif dalam konteks internasional.\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Generate teks dari model AgroLensGPT berbasis prompt dan tokenizer.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Prompt input dari user.\n",
    "        max_new_tokens (int): Jumlah maksimum token baru yang dihasilkan.\n",
    "\n",
    "    Returns:\n",
    "        str: Hasil teks yang dihasilkan oleh model.\n",
    "    \"\"\"\n",
    "    # Tokenisasi prompt dan potong jika terlalu panjang\n",
    "    input_ids = tokenizer.encode(prompt).ids[:MAX_LEN]\n",
    "    input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Prediksi logit dari model\n",
    "        logits = model(input_tensor, training=False)\n",
    "\n",
    "        # Ambil token berikutnya (greedy decoding)\n",
    "        next_token = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "\n",
    "        # Gabungkan token ke input\n",
    "        input_tensor = tf.concat(\n",
    "            [input_tensor, tf.expand_dims(next_token, axis=1)], axis=1\n",
    "        )\n",
    "\n",
    "        # Stop jika token </s> ditemukan\n",
    "        if next_token.numpy()[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "\n",
    "        # Stop jika melebihi panjang maksimum\n",
    "        if input_tensor.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "\n",
    "    # Decode seluruh output menjadi teks\n",
    "    output_ids = input_tensor[0].numpy().tolist()\n",
    "    return tokenizer.decode(output_ids)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"\"\"<s> Berikut adalah informasi terkait:\n",
    "...\n",
    "Pertanyaan: Apa itu penyakit blast?\n",
    "Jawaban: Blast disebabkan oleh jamur...\n",
    "</s>\n",
    "\"\"\"\n",
    "    print(\"🌾 AgroLens Menjawab:\")\n",
    "    print(generate(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721b5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class AgroTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, vocab_size=8000, max_length=128, d_model=256, n_heads=4, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.pos_embed = layers.Embedding(input_dim=max_length, output_dim=d_model)\n",
    "\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(d_model * 4, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(d_model),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        pos = tf.expand_dims(pos, 0)\n",
    "        x = self.token_embed(x) + self.pos_embed(pos)\n",
    "\n",
    "        attn_output = self.attn(x, x, attention_mask=None, use_causal_mask=False)\n",
    "        x = self.ln1(x + self.dropout(attn_output, training=training))\n",
    "\n",
    "        ffn_output = self.ffn(x, training=training)\n",
    "        x = self.ln2(x + self.dropout(ffn_output, training=training))\n",
    "\n",
    "        x = tf.reduce_mean(x, axis=1)  # Global average pooling\n",
    "        return x  # shape: (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140283c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'agro_lens_gpt_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'agro_transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌾 AgroLens RAG Menjawab:\n",
      "[Context 1] Pengendalian hayati untuk penyakit blast bersifat lebih ramah lingkungan dan berbiaya rendah dibandi...\n",
      "[Context 2] Petani dapat menerapkan strategi seperti budidaya campuran varietas padi (multilines), penggunaan va...\n",
      "[Context 3] Ya, musim tanam yang jatuh di musim hujan atau transisi hujan-kemarau meningkatkan risiko blast, kar...\n",
      " Berikut adalah informasi terkait\n",
      "Pengendalian hayati untuk penyakit blast bersifat lebih ramah lingkungan dan berbiaya rendah dibandingkan penggunaan fungisida kimia. Agen hayati tidak mencemari lingkungan dan bisa menjadi alternatif yang berkelanjutan dalam jangka panjang.\n",
      "Petani dapat menerapkan strategi seperti budidaya campuran varietas padi (multilines), penggunaan varietas tahan spektrum luas, serta pengurangan ketergantungan pada fungisida melalui pendekatan rekayasa ekologi. Strategi ini meningkatkan hasil dan ketahanan penyakit secara berkelanjutan.\n",
      "Ya, musim tanam yang jatuh di musim hujan atau transisi hujan-kemarau meningkatkan risiko blast, karena spora jamur menyebar lebih cepat di kondisi lembab dan berembun.\n",
      "\n",
      "Pertanyaan Apa itu penyakit blast?\n",
      "Jawaban bu sorokiniana� lokus endemeringan pada merangs eradikasi kut jugaarah dunangat Peralihan selub\n"
     ]
    }
   ],
   "source": [
    "RETRIEVER_WEIGHTS = \"retriever_encoder_weights.weights.h5\"\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "# === Load GPT model ===\n",
    "model = AgroLensGPT(vocab_size=VOCAB_SIZE, max_length=MAX_LEN)\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "model.load_weights(MODEL_PATH)\n",
    "# === Load retriever encoder ===\n",
    "retriever_encoder = AgroTransformerEncoder(vocab_size=VOCAB_SIZE, max_length=MAX_LEN)\n",
    "retriever_encoder.build(input_shape=(None, MAX_LEN))\n",
    "retriever_encoder.load_weights(RETRIEVER_WEIGHTS)\n",
    "\n",
    "# === Load corpus passages (e.g. response list) ===\n",
    "with open(\"dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [json.loads(line)[\"response\"] for line in f if \"response\" in line]\n",
    "\n",
    "\n",
    "with open(\"rag_corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def detect_disease(prompt: str):\n",
    "    p = prompt.lower()\n",
    "    if \"blast\" in p:\n",
    "        return \"blast\"\n",
    "    elif \"tungro\" in p:\n",
    "        return \"tungro\"\n",
    "    # Tambahkan deteksi lain\n",
    "    return None\n",
    "\n",
    "\n",
    "def retrieve_top_k_contexts(query: str, top_k=3):\n",
    "    disease = detect_disease(query)\n",
    "    filtered = [c for c in corpus if c[\"disease\"] == disease]\n",
    "\n",
    "    query_ids = tokenizer.encode(query).ids[:MAX_LEN]\n",
    "    q_tensor = tf.constant([query_ids], dtype=tf.int32)\n",
    "    q_embed = retriever_encoder(q_tensor)\n",
    "\n",
    "    passage_embeddings = []\n",
    "    texts = []\n",
    "    for entry in filtered:\n",
    "        ids = tokenizer.encode(entry[\"text\"]).ids[:MAX_LEN]\n",
    "        t_tensor = tf.constant([ids], dtype=tf.int32)\n",
    "        emb = retriever_encoder(t_tensor)\n",
    "        passage_embeddings.append(emb[0].numpy())\n",
    "        texts.append(entry[\"text\"])\n",
    "\n",
    "    sims = tf.linalg.matmul(q_embed, tf.transpose(tf.constant(passage_embeddings)))\n",
    "    top_idx = tf.math.top_k(sims, k=min(top_k, len(texts))).indices.numpy()[0]\n",
    "    return [texts[i] for i in top_idx]\n",
    "\n",
    "\n",
    "# === Generate with GPT + context ===\n",
    "def generate_rag(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    top_contexts = retrieve_top_k_contexts(prompt)\n",
    "    for i, ctx in enumerate(top_contexts):\n",
    "        print(f\"[Context {i+1}] {ctx[:100]}...\")\n",
    "    full_prompt = (\n",
    "        \"Berikut adalah informasi terkait:\\n\"\n",
    "        + \"\\n\".join(top_contexts)\n",
    "        + f\"\\n\\nPertanyaan: {prompt}\\nJawaban:\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer.encode(full_prompt).ids[:MAX_LEN]\n",
    "    input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # logits.shape: [1, seq_len, vocab_size]\n",
    "        logits = model(input_tensor, training=False)\n",
    "        logits = logits[:, -1, :]  # ambil logit terakhir, shape: [1, vocab_size]\n",
    "\n",
    "        # Top-k sampling\n",
    "        k = 10\n",
    "        values, indices = tf.math.top_k(logits, k=k)  # [1, k]\n",
    "        next_token = tf.random.categorical(values, num_samples=1)  # [1, 1]\n",
    "        next_token = tf.gather(indices, next_token, batch_dims=1)  # [1, 1]\n",
    "        input_tensor = tf.concat(\n",
    "            [input_tensor, next_token], axis=1\n",
    "        )  # now both [1, seq_len]\n",
    "\n",
    "        if next_token.numpy()[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "        if input_tensor.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "\n",
    "    output_ids = input_tensor[0].numpy().tolist()\n",
    "    return tokenizer.decode(output_ids)\n",
    "\n",
    "\n",
    "# === Run test ===\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Apa itu penyakit blast?\"\n",
    "    print(\"🌾 AgroLens RAG Menjawab:\")\n",
    "    print(generate_rag(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454b4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53524a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170545fbea82488f8af2a4d0aac1fde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Apa itu penyakit blast?', description='❓ Pertanyaan:', layout=Layout(width='100%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    value=\"Apa itu penyakit blast?\",\n",
    "    placeholder=\"Tulis pertanyaan di sini...\",\n",
    "    description=\"❓ Pertanyaan:\",\n",
    "    layout=widgets.Layout(width=\"100%\"),\n",
    ")\n",
    "\n",
    "output_box = widgets.Output()\n",
    "generate_button = widgets.Button(\n",
    "    description=\"Jawab 🚀\", button_style=\"success\", layout=widgets.Layout(width=\"15%\")\n",
    ")\n",
    "\n",
    "\n",
    "def on_generate_clicked(b):\n",
    "    prompt = input_box.value\n",
    "    response = rag_generate(prompt)\n",
    "    output_box.clear_output()\n",
    "    with output_box:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"### 🧑 Kamu: \\n{prompt}\\n---\\n### 🌾 AgroLens Menjawab:\\n{response}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "generate_button.on_click(on_generate_clicked)\n",
    "\n",
    "# Tampilkan\n",
    "display(widgets.VBox([input_box, generate_button, output_box]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2f5d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip <command> [options]\n",
      "\n",
      "Commands:\n",
      "  install                     Install packages.\n",
      "  download                    Download packages.\n",
      "  uninstall                   Uninstall packages.\n",
      "  freeze                      Output installed packages in requirements format.\n",
      "  inspect                     Inspect the python environment.\n",
      "  list                        List installed packages.\n",
      "  show                        Show information about installed packages.\n",
      "  check                       Verify installed packages have compatible dependencies.\n",
      "  config                      Manage local and global configuration.\n",
      "  search                      Search PyPI for packages.\n",
      "  cache                       Inspect and manage pip's wheel cache.\n",
      "  index                       Inspect information available from package indexes.\n",
      "  wheel                       Build wheels from your requirements.\n",
      "  hash                        Compute hashes of package archives.\n",
      "  completion                  A helper command used for command completion.\n",
      "  debug                       Show information useful for debugging.\n",
      "  help                        Show help for commands.\n",
      "\n",
      "General Options:\n",
      "  -h, --help                  Show help.\n",
      "  --debug                     Let unhandled exceptions propagate outside the\n",
      "                              main subroutine, instead of logging them to\n",
      "                              stderr.\n",
      "  --isolated                  Run pip in an isolated mode, ignoring\n",
      "                              environment variables and user configuration.\n",
      "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
      "                              exit with an error otherwise.\n",
      "  --python <python>           Run pip with the specified Python interpreter.\n",
      "  -v, --verbose               Give more output. Option is additive, and can be\n",
      "                              used up to 3 times.\n",
      "  -V, --version               Show version and exit.\n",
      "  -q, --quiet                 Give less output. Option is additive, and can be\n",
      "                              used up to 3 times (corresponding to WARNING,\n",
      "                              ERROR, and CRITICAL logging levels).\n",
      "  --log <path>                Path to a verbose appending log.\n",
      "  --no-input                  Disable prompting for input.\n",
      "  --keyring-provider <keyring_provider>\n",
      "                              Enable the credential lookup via the keyring\n",
      "                              library if user input is allowed. Specify which\n",
      "                              mechanism to use [disabled, import, subprocess].\n",
      "                              (default: disabled)\n",
      "  --proxy <proxy>             Specify a proxy in the form\n",
      "                              scheme://[user:passwd@]proxy.server:port.\n",
      "  --retries <retries>         Maximum number of retries each connection should\n",
      "                              attempt (default 5 times).\n",
      "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
      "  --exists-action <action>    Default action when a path already exists:\n",
      "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
      "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
      "                              even though it does not have valid or any HTTPS.\n",
      "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
      "                              provided, overrides the default. See 'SSL\n",
      "                              Certificate Verification' in pip documentation\n",
      "                              for more information.\n",
      "  --client-cert <path>        Path to SSL client certificate, a single file\n",
      "                              containing the private key and the certificate\n",
      "                              in PEM format.\n",
      "  --cache-dir <dir>           Store the cache data in <dir>.\n",
      "  --no-cache-dir              Disable the cache.\n",
      "  --disable-pip-version-check\n",
      "                              Don't periodically check PyPI to determine\n",
      "                              whether a new version of pip is available for\n",
      "                              download. Implied with --no-index.\n",
      "  --no-color                  Suppress colored output.\n",
      "  --no-python-version-warning\n",
      "                              Silence deprecation warnings for upcoming\n",
      "                              unsupported Pythons.\n",
      "  --use-feature <feature>     Enable new functionality, that may be backward\n",
      "                              incompatible.\n",
      "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
      "                              removed in the future.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
