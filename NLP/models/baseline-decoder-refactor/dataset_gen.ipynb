{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecda103d",
   "metadata": {},
   "source": [
    "## **Generate Dataset for Main.ipynb in this directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3fd041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total 375 baris data berhasil dimuat dari Excel.\n",
      "\n",
      "--- Membuat Triplet Dataset ---\n",
      "✅ Berhasil menyimpan 375 data ke 'triplet_dataset_with_soft_negative.jsonl'\n",
      "\n",
      "--- Membuat Standard Prompt-Response Dataset ---\n",
      "✅ Berhasil menyimpan 375 data ke 'dataset_standard.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path # Gunakan pathlib untuk manajemen path yang lebih baik\n",
    "\n",
    "# --- (Salin semua fungsi Anda: load_combined_dataset, generate_triplets, save_triplets_to_jsonl) ---\n",
    "def load_combined_dataset(excel_path, sheet_names):\n",
    "    all_data = []\n",
    "    if not Path(excel_path).exists():\n",
    "        print(f\"❌ Error: File Excel tidak ditemukan di path '{excel_path}'\")\n",
    "        return pd.DataFrame()\n",
    "    for sheet in sheet_names:\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "            df = df.rename(columns=str.lower)\n",
    "            if \"prompt\" in df.columns and \"response\" in df.columns and not df.empty:\n",
    "                df = df.dropna(subset=['prompt', 'response'])\n",
    "                if not df.empty:\n",
    "                    df[\"penyakit\"] = sheet.lower()\n",
    "                    all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saat membaca sheet '{sheet}': {e}\")\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "def generate_triplets(df, n_negatives=2, add_soft_negative=True, seed=42):\n",
    "    if df.empty: return []\n",
    "    triplets = []\n",
    "    for idx, row in df.iterrows():\n",
    "        query, positive, penyakit, topik = row[\"prompt\"], row[\"response\"], row[\"penyakit\"], row.get(\"topik\")\n",
    "        hard_pool = df[df[\"penyakit\"] != penyakit]\n",
    "        hard_negatives = hard_pool[\"response\"].dropna().sample(n=min(n_negatives, len(hard_pool)), random_state=seed + idx).tolist()\n",
    "        soft_negative = None\n",
    "        if add_soft_negative and topik and 'topik' in df.columns:\n",
    "            soft_pool = df[(df[\"penyakit\"] == penyakit) & (df[\"topik\"] != topik) & (df[\"response\"].notna())]\n",
    "            if not soft_pool.empty:\n",
    "                soft_negative = soft_pool.sample(n=1, random_state=seed + idx)[\"response\"].iloc[0]\n",
    "        negatives = hard_negatives\n",
    "        if soft_negative: negatives.append(soft_negative)\n",
    "        triplets.append({\"query\": query, \"positive\": positive, \"negatives\": negatives})\n",
    "    return triplets\n",
    "\n",
    "def save_to_jsonl(data_list, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data_list:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ======== PARAMETER ========\n",
    "excel_path = \"../../datasets/xlsx/Dataset Labeling Chatbot.xlsx\"\n",
    "sheet_names = [\"Blast\", \"Tungro\", \"Bacterial Blight\", \"BrownSpot\", \"LeafScald\"]\n",
    "output_file_triplet = \"triplet_dataset_with_soft_negative.jsonl\"\n",
    "output_file_standard = \"dataset_standard.jsonl\" # Nama file untuk dataset standar\n",
    "# ==============================================================================\n",
    "\n",
    "# Menjalankan proses utama\n",
    "df_combined = load_combined_dataset(excel_path, sheet_names)\n",
    "\n",
    "# Lanjutkan hanya jika data berhasil dimuat\n",
    "if not df_combined.empty:\n",
    "    print(f\"\\n✅ Total {len(df_combined)} baris data berhasil dimuat dari Excel.\")\n",
    "    \n",
    "    # --- 1. Generate dan Simpan TRIPLET Dataset ---\n",
    "    print(\"\\n--- Membuat Triplet Dataset ---\")\n",
    "    triplets = generate_triplets(df_combined, n_negatives=2, add_soft_negative=True)\n",
    "    save_to_jsonl(triplets, output_file_triplet)\n",
    "    print(f\"✅ Berhasil menyimpan {len(triplets)} data ke '{output_file_triplet}'\")\n",
    "\n",
    "    # --- 2. Generate dan Simpan STANDARD Dataset (dari sumber yang sama) ---\n",
    "    print(\"\\n--- Membuat Standard Prompt-Response Dataset ---\")\n",
    "    standard_data = []\n",
    "    for idx, row in df_combined.iterrows():\n",
    "        prompt = row.get(\"prompt\", \"\").strip()\n",
    "        response = row.get(\"response\", \"\").strip()\n",
    "        if prompt and response:\n",
    "            standard_data.append({\"prompt\": prompt, \"response\": response})\n",
    "    \n",
    "    save_to_jsonl(standard_data, output_file_standard)\n",
    "    print(f\"✅ Berhasil menyimpan {len(standard_data)} data ke '{output_file_standard}'\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nProses dihentikan karena data awal tidak berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129b990",
   "metadata": {},
   "source": [
    "## **Experiment RAG Failed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9edb928a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisease\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisease\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrag_data\u001b[49m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_corpus.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m corpus:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_data' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = [{\"text\": item[\"response\"], \"disease\": item[\"disease\"]} for item in rag_data]\n",
    "with open(\"rag_corpus.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in corpus:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d32d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:41.530372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-05 21:59:41.539939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749131981.550126   21636 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749131981.553040   21636 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749131981.559932   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749131981.559949   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749131981.559950   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749131981.559950   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-05 21:59:41.562434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec6d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "MAX_LEN = 128\n",
    "D_MODEL = 256\n",
    "N_HEADS = 4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d98461",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\n",
    "    \"tokenizer-agrolens.json\"\n",
    ")  # ganti sesuai path tokenizer kamu\n",
    "def encode(text):\n",
    "    ids = tokenizer.encode(text).ids[:MAX_LEN]\n",
    "    ids = ids + [0] * (MAX_LEN - len(ids))  # padding\n",
    "    return tf.constant(ids, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7242cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749131984.010201   21636 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            yield {\n",
    "                \"query_input\": encode(ex[\"query\"]),\n",
    "                \"passage_input\": encode(ex[\"positive\"]),\n",
    "            }\n",
    "\n",
    "\n",
    "def get_tf_dataset(path):\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: load_dataset(path),\n",
    "        output_signature={\n",
    "            \"query_input\": tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n",
    "            \"passage_input\": tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n",
    "        },\n",
    "    )\n",
    "    return ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset = get_tf_dataset(\"triplet_dataset_with_soft_negative.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d176f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class AgroTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, vocab_size=8000, max_length=128, d_model=256, n_heads=4, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.pos_embed = layers.Embedding(input_dim=max_length, output_dim=d_model)\n",
    "\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(d_model * 4, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(d_model),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        pos = tf.expand_dims(pos, 0)\n",
    "        x = self.token_embed(x) + self.pos_embed(pos)\n",
    "\n",
    "        attn_output = self.attn(x, x, attention_mask=None, use_causal_mask=False)\n",
    "        x = self.ln1(x + self.dropout(attn_output, training=training))\n",
    "\n",
    "        ffn_output = self.ffn(x, training=training)\n",
    "        x = self.ln2(x + self.dropout(ffn_output, training=training))\n",
    "\n",
    "        x = tf.reduce_mean(x, axis=1)  # Global average pooling\n",
    "        return x  # shape: (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c67585",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AgroTransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbfe14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(query_embed, passage_embed):\n",
    "    sim_matrix = tf.matmul(query_embed, passage_embed, transpose_b=True)  # (B, B)\n",
    "    labels = tf.range(tf.shape(sim_matrix)[0])\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, sim_matrix, from_logits=True\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a33c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:51.372563: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.2228\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:52.507434: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7777\n",
      "Epoch 3/5\n",
      "Loss: 2.7653\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:54.748732: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7509\n",
      "Epoch 5/5\n",
      "Loss: 2.7135\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    epoch_loss = []\n",
    "    for batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_embed = encoder(batch[\"query_input\"], training=True)\n",
    "            p_embed = encoder(batch[\"passage_input\"], training=True)\n",
    "            loss = contrastive_loss(q_embed, p_embed)\n",
    "        grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "        epoch_loss.append(loss.numpy())\n",
    "\n",
    "    print(f\"Loss: {np.mean(epoch_loss):.4f}\")\n",
    "\n",
    "# ======= 8. Save Encoder =======\n",
    "encoder.save_weights(\"retriever_encoder_weights.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
