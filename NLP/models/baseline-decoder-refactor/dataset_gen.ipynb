{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fd041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triplet dataset with soft negatives saved to: triplet_dataset_with_soft_negative.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def load_combined_dataset(excel_path, sheet_names):\n",
    "    all_data = []\n",
    "    for sheet in sheet_names:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        df = df.rename(columns=str.lower)\n",
    "        df[\"penyakit\"] = sheet.lower()\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "def generate_triplets(df, n_negatives=2, add_soft_negative=True, seed=42):\n",
    "    triplets = []\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row[\"prompt\"]\n",
    "        positive = row[\"response\"]\n",
    "        penyakit = row[\"penyakit\"]\n",
    "        topik = row.get(\"topik\", None)\n",
    "\n",
    "        # Hard negatives dari penyakit lain\n",
    "        hard_pool = df[df[\"penyakit\"] != penyakit]\n",
    "        hard_negatives = (\n",
    "            hard_pool[\"response\"]\n",
    "            .dropna()\n",
    "            .sample(n=min(n_negatives, len(hard_pool)), random_state=seed)\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # Soft negative dari penyakit sama tapi topik beda\n",
    "        soft_negative = None\n",
    "        if add_soft_negative and topik:\n",
    "            soft_pool = df[\n",
    "                (df[\"penyakit\"] == penyakit)\n",
    "                & (df[\"topik\"] != topik)\n",
    "                & (df[\"response\"].notna())\n",
    "            ]\n",
    "            if not soft_pool.empty:\n",
    "                soft_negative = soft_pool.sample(n=1, random_state=seed)[\n",
    "                    \"response\"\n",
    "                ].iloc[0]\n",
    "\n",
    "        # Gabungkan negatives\n",
    "        negatives = hard_negatives\n",
    "        if soft_negative:\n",
    "            negatives.append(soft_negative)\n",
    "\n",
    "        triplets.append({\"query\": query, \"positive\": positive, \"negatives\": negatives})\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def save_triplets_to_jsonl(triplets, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in triplets:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ======== PARAMETER ========\n",
    "excel_path = \"../../datasets/xlsx/Dataset Labeling Chatbot.xlsx\"\n",
    "sheet_names = [\"Blast\", \"Tungro\"]\n",
    "output_file = \"triplet_dataset_with_soft_negative.jsonl\"\n",
    "# ===========================\n",
    "\n",
    "df = load_combined_dataset(excel_path, sheet_names)\n",
    "triplets = generate_triplets(df, n_negatives=2, add_soft_negative=  True)\n",
    "save_triplets_to_jsonl(triplets, output_file)\n",
    "\n",
    "print(f\"✅ Triplet dataset with soft negatives saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dataset_ragstyle.jsonl berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# === Load original dataset ===\n",
    "with open(\"dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# === Format standar: prompt + response saja ===\n",
    "standard_data = []\n",
    "for item in data:\n",
    "    prompt = item[\"prompt\"].strip()\n",
    "    response = item[\"response\"].strip()\n",
    "    standard_data.append({\"prompt\": prompt, \"response\": response})\n",
    "\n",
    "# === Simpan ke file JSONL ===\n",
    "with open(\"dataset_standard.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in standard_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ dataset_standard.jsonl berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9edb928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [{\"text\": item[\"response\"], \"disease\": item[\"disease\"]} for item in rag_data]\n",
    "with open(\"rag_corpus.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in corpus:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d32d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:41.530372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-05 21:59:41.539939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749131981.550126   21636 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749131981.553040   21636 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749131981.559932   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749131981.559949   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749131981.559950   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749131981.559950   21636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-05 21:59:41.562434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec6d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "MAX_LEN = 128\n",
    "D_MODEL = 256\n",
    "N_HEADS = 4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d98461",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\n",
    "    \"tokenizer-agrolens.json\"\n",
    ")  # ganti sesuai path tokenizer kamu\n",
    "def encode(text):\n",
    "    ids = tokenizer.encode(text).ids[:MAX_LEN]\n",
    "    ids = ids + [0] * (MAX_LEN - len(ids))  # padding\n",
    "    return tf.constant(ids, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7242cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749131984.010201   21636 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            yield {\n",
    "                \"query_input\": encode(ex[\"query\"]),\n",
    "                \"passage_input\": encode(ex[\"positive\"]),\n",
    "            }\n",
    "\n",
    "\n",
    "def get_tf_dataset(path):\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: load_dataset(path),\n",
    "        output_signature={\n",
    "            \"query_input\": tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n",
    "            \"passage_input\": tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n",
    "        },\n",
    "    )\n",
    "    return ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset = get_tf_dataset(\"triplet_dataset_with_soft_negative.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d176f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class AgroTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, vocab_size=8000, max_length=128, d_model=256, n_heads=4, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.pos_embed = layers.Embedding(input_dim=max_length, output_dim=d_model)\n",
    "\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(d_model * 4, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(d_model),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        pos = tf.expand_dims(pos, 0)\n",
    "        x = self.token_embed(x) + self.pos_embed(pos)\n",
    "\n",
    "        attn_output = self.attn(x, x, attention_mask=None, use_causal_mask=False)\n",
    "        x = self.ln1(x + self.dropout(attn_output, training=training))\n",
    "\n",
    "        ffn_output = self.ffn(x, training=training)\n",
    "        x = self.ln2(x + self.dropout(ffn_output, training=training))\n",
    "\n",
    "        x = tf.reduce_mean(x, axis=1)  # Global average pooling\n",
    "        return x  # shape: (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c67585",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AgroTransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbfe14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(query_embed, passage_embed):\n",
    "    sim_matrix = tf.matmul(query_embed, passage_embed, transpose_b=True)  # (B, B)\n",
    "    labels = tf.range(tf.shape(sim_matrix)[0])\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, sim_matrix, from_logits=True\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a33c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:51.372563: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.2228\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:52.507434: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7777\n",
      "Epoch 3/5\n",
      "Loss: 2.7653\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 21:59:54.748732: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7509\n",
      "Epoch 5/5\n",
      "Loss: 2.7135\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    epoch_loss = []\n",
    "    for batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_embed = encoder(batch[\"query_input\"], training=True)\n",
    "            p_embed = encoder(batch[\"passage_input\"], training=True)\n",
    "            loss = contrastive_loss(q_embed, p_embed)\n",
    "        grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "        epoch_loss.append(loss.numpy())\n",
    "\n",
    "    print(f\"Loss: {np.mean(epoch_loss):.4f}\")\n",
    "\n",
    "# ======= 8. Save Encoder =======\n",
    "encoder.save_weights(\"retriever_encoder_weights.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
