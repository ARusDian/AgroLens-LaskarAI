{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a99c492",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdc6f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.6' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'd:/laragon/bin/python/python-3.10/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import json\n",
    "\n",
    "# === Load data ===\n",
    "with open(\"baseline-dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = []\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"prompt\"])\n",
    "        texts.append(data[\"response\"])\n",
    "\n",
    "# === Write to plain text (required by tokenizer trainer) ===\n",
    "with open(\"tokenizer_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(text.strip() + \"\\n\")\n",
    "\n",
    "# === Init tokenizer ===\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"],\n",
    ")\n",
    "\n",
    "# === Train ===\n",
    "tokenizer.train([\"tokenizer_corpus.txt\"], trainer)\n",
    "\n",
    "# === Post-processing untuk auto menambahkan <s> dan </s> saat encoding\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> <s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# === Save ===\n",
    "tokenizer.save(\"tokenizer-agrolens.json\")   \n",
    "print(\"✅ Tokenizer saved to tokenizer-agrolens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d359a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer-agrolens.json\")\n",
    "enc = tokenizer.encode(\"Apa itu penyakit blast?\")\n",
    "print(enc.tokens)\n",
    "print(enc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0844",
   "metadata": {},
   "source": [
    "# GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class AgroLensGPT(tf.keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_length=512,\n",
    "        d_model=256,\n",
    "        n_heads=4,\n",
    "        n_layers=4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(AgroLensGPT, self).__init__()\n",
    "        self.token_embed = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.pos_embed = layers.Embedding(input_dim=max_length, output_dim=d_model)\n",
    "\n",
    "        self.decoder_blocks = [\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-6),\n",
    "        ]\n",
    "        self.decoder_layers = []\n",
    "        for _ in range(n_layers):\n",
    "            self.decoder_layers.append(\n",
    "                [\n",
    "                    layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model),\n",
    "                    layers.Dropout(dropout),\n",
    "                    layers.LayerNormalization(epsilon=1e-6),\n",
    "                    layers.Dense(d_model * 4, activation=\"relu\"),\n",
    "                    layers.Dense(d_model),\n",
    "                    layers.Dropout(dropout),\n",
    "                    layers.LayerNormalization(epsilon=1e-6),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.final_ln = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.output_head = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B, T = tf.shape(x)[0], tf.shape(x)[1]\n",
    "        token_emb = self.token_embed(x)  # (B, T, d_model)\n",
    "        positions = tf.range(start=0, limit=T, delta=1)\n",
    "        pos_emb = self.pos_embed(positions)  # (T, d_model)\n",
    "        pos_emb = tf.expand_dims(pos_emb, axis=0)  # (1, T, d_model)\n",
    "        h = token_emb + pos_emb  # (B, T, d_model)\n",
    "\n",
    "        # Causal mask\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((T, T)), -1, 0)  # (T, T)\n",
    "        causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
    "\n",
    "        for mha, drop1, ln1, ff1, ff2, drop2, ln2 in self.decoder_layers:\n",
    "            attn_output = mha(h, h, h, attention_mask=causal_mask, use_causal_mask=True)\n",
    "            attn_output = drop1(attn_output, training=training)\n",
    "            h = ln1(h + attn_output)\n",
    "\n",
    "            ffn_output = ff1(h)\n",
    "            ffn_output = ff2(ffn_output)\n",
    "            ffn_output = drop2(ffn_output, training=training)\n",
    "            h = ln2(h + ffn_output)\n",
    "\n",
    "        h = self.final_ln(h)\n",
    "        logits = self.output_head(h)  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgroLensGPT(vocab_size=8000)\n",
    "display(model.summary())\n",
    "sample_input = tf.random.uniform((2, 64), minval=0, maxval=8000, dtype=tf.int32)\n",
    "logits = model(sample_input)\n",
    "print(logits.shape)  # Expected: (2, 64, 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ad101",
   "metadata": {},
   "source": [
    "# Loader dan Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "class AgroDatasetTF(tf.data.Dataset):\n",
    "    def __new__(cls, path, tokenizer_path, max_len=256):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        samples = []\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                prompt = data[\"prompt\"]\n",
    "                response = data[\"response\"]\n",
    "                combined = f\"{prompt} {response}\"\n",
    "\n",
    "                # Tokenize and truncate\n",
    "                ids = tokenizer.encode(combined).ids[:max_len]\n",
    "\n",
    "                if len(ids) >= 2:  # minimal length to create input/label\n",
    "                    input_ids = ids[:-1]\n",
    "                    labels = ids[1:]\n",
    "                    samples.append((input_ids, labels))\n",
    "\n",
    "        # Convert to TensorFlow tensors\n",
    "        def gen():\n",
    "            for input_ids, labels in samples:\n",
    "                yield {\n",
    "                    \"input_ids\": tf.constant(input_ids, dtype=tf.int32),\n",
    "                    \"labels\": tf.constant(labels, dtype=tf.int32),\n",
    "                }\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_signature={\n",
    "                \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "                \"labels\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe55cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AgroDatasetTF(\n",
    "    \"baseline-dataset.jsonl\", \"tokenizer-agrolens.json\", max_len=512\n",
    ")\n",
    "dataset = dataset.padded_batch(8, padded_shapes={\"input_ids\": [None], \"labels\": [None]})\n",
    "for batch in dataset.take(1):\n",
    "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8618d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LR = 3e-4\n",
    "MAX_LEN = 512\n",
    "\n",
    "# --- Dataset ---\n",
    "dataset = AgroDatasetTF(\n",
    "    \"baseline-dataset.jsonl\", \"tokenizer-agrolens.json\", max_len=MAX_LEN\n",
    ")\n",
    "dataset = dataset.padded_batch(\n",
    "    BATCH_SIZE, padded_shapes={\"input_ids\": [None], \"labels\": [None]}\n",
    ")\n",
    "dataset = dataset.shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- Model ---\n",
    "model = AgroLensGPT(vocab_size=8000)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Custom Training Loop ---\n",
    "@tf.function\n",
    "def train_step(input_ids, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, training=True)\n",
    "        # Shifted label loss, ignoring padding (-100 equivalent in PyTorch)\n",
    "        mask = tf.cast(labels != -100, tf.float32)\n",
    "        loss_values = loss_fn(labels, logits)\n",
    "        loss = tf.reduce_sum(loss_values * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for batch in dataset:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss = train_step(input_ids, labels)\n",
    "        total_loss += loss.numpy()\n",
    "        steps += 1\n",
    "\n",
    "    avg_loss = total_loss / steps\n",
    "    print(f\"📘 Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# --- Save weights ---\n",
    "model.save_weights(\"agrolens_model_tf.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28847bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# --- Konstanta dan Load Model ---\n",
    "MODEL_PATH = \"agrolens_model_tf.weights.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer-agrolens.json\"\n",
    "VOCAB_SIZE = 8000\n",
    "MAX_LEN = 128\n",
    "\n",
    "model = AgroLensGPT(vocab_size=VOCAB_SIZE)\n",
    "dummy_input = tf.constant([[1] * 64], dtype=tf.int32)  # bentuk (1, 64)\n",
    "_ = model(dummy_input)  # memanggil forward p\n",
    "model.load_weights(MODEL_PATH)\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, max_new_tokens=50):\n",
    "    # Tokenisasi prompt\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    input_tensor = tf.constant([input_ids], dtype=tf.int32)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Loloskan input ke model\n",
    "        logits = model(input_tensor, training=False)\n",
    "\n",
    "        # Ambil token berikutnya dari distribusi logit terakhir\n",
    "        next_token = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "\n",
    "        # Tambahkan ke input_tensor\n",
    "        input_tensor = tf.concat(\n",
    "            [input_tensor, tf.expand_dims(next_token, axis=1)], axis=1\n",
    "        )\n",
    "\n",
    "        # Jika token akhir ditemukan\n",
    "        if tokenizer.token_to_id(\"</s>\") in next_token.numpy():\n",
    "            break\n",
    "\n",
    "        # Batasi panjang maksimum\n",
    "        if input_tensor.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "\n",
    "    # Decode output\n",
    "    output_ids = input_tensor[0].numpy().tolist()\n",
    "    return tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53524a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    value=\"Apa itu penyakit blast?\",\n",
    "    placeholder=\"Tulis pertanyaan di sini...\",\n",
    "    description=\"❓ Pertanyaan:\",\n",
    "    layout=widgets.Layout(width=\"100%\"),\n",
    ")\n",
    "\n",
    "output_box = widgets.Output()\n",
    "generate_button = widgets.Button(\n",
    "    description=\"Jawab 🚀\", button_style=\"success\", layout=widgets.Layout(width=\"15%\")\n",
    ")\n",
    "\n",
    "\n",
    "def on_generate_clicked(b):\n",
    "    prompt = input_box.value\n",
    "    response = generate(prompt)\n",
    "    output_box.clear_output()\n",
    "    with output_box:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"### 🧑 Kamu: \\n{prompt}\\n---\\n### 🌾 AgroLens Menjawab:\\n{response}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "generate_button.on_click(on_generate_clicked)\n",
    "\n",
    "# Tampilkan\n",
    "display(widgets.VBox([input_box, generate_button, output_box]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
